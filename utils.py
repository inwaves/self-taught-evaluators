import pandas as pd
from datasets import Dataset
from prompts import (
    PROMPT_TO_GENERATE_MODIFIED_INSTRUCTION_AND_REJECTED_RESPONSE,
    PROMPT_TO_GENERATE_CHOSEN_RESPONSE,
    PROMPT_TO_GENERATE_JUDGEMENT_ANNOTATION,
    PROMPT_TO_CATEGORISE_INPUTS
)
import re


def generate_chosen_response(model, user_instruction: str) -> str:
    """Given an original prompt x_i containing user instructions,
    this function generates a chosen response y^w_i.
    args:
        model (BaseModel): the model to use for generation
        user_instruction (str): the original prompt x_i
    return:
        chosen_response (str): the chosen response y^w_i
    """
    assembled_prompt = PROMPT_TO_GENERATE_CHOSEN_RESPONSE.format(
        instruction=user_instruction,
    )
    raw_response = model.generate(assembled_prompt)

    # TODO: extract the chosen response from the raw response.
    chosen_response = raw_response
    return chosen_response


def generate_modified_instruction_and_rejected_response(model, original_prompt: str, chosen_response: str) -> tuple[str, str]:
    """Given an original prompt x_i containing user instructions,
    this function first generates a modified instruction x'_i, and then 
    generates a response y^l_i which is designed to be worse 
    than y^w_i, the chosen response to the original prompt x_i.
    args:
        model (BaseModel): the model to use for generation
        original_prompt (str): the original prompt x_i
    return:
        modified_instruction (str): the modified instruction x'_i
        rejected_response (str): the rejected response y^l_i
    """

    assembled_prompt = PROMPT_TO_GENERATE_MODIFIED_INSTRUCTION_AND_REJECTED_RESPONSE.format(
        instruction=original_prompt,
        baseline_response=chosen_response,
    )
    raw_response = model.generate(assembled_prompt)

    # TODO: extract the modified instruction and rejected response from the raw response.
    modified_instruction = raw_response
    rejected_response = raw_response

    return modified_instruction, rejected_response


def generate_response_pair_and_modified_instruction(model, user_instruction: str) -> tuple[str, str, str]:
    chosen = generate_chosen_response(user_instruction)
    modified_instruction, rejected = generate_modified_instruction_and_rejected_response(model, user_instruction)
    return chosen, rejected, modified_instruction


def generate_judgement(model, prompt: str) -> str:
    """
    Generate a judgement based on the given prompt using the provided model.

    This function generates a response using the model, then attempts to extract
    a judgement (either 'A' or 'B') from the response. It first looks for an exact
    match using regex, then tries to infer the judgement from the text if no exact
    match is found. If no judgement can be determined, it raises a ValueError.

    Args:
        model: The model used to generate the response.
        prompt (str): The input prompt for generating the judgement.

    Returns:
        str: The extracted judgement, either 'A' or 'B'.

    Raises:
        ValueError: If no valid judgement can be extracted from the response.
    """
    raw_response = model.generate(prompt)

    # Use regex to find the judgement
    match = re.search(r'\[\[([AB])\]\]', raw_response)
    
    if match:
        judgement = match.group(1)
    else:
        # If no exact match is found, try to infer the judgement
        lower_response = raw_response.lower()
        if "assistant a is better" in lower_response:
            judgement = "A"
        elif "assistant b is better" in lower_response:
            judgement = "B"
        else:
            # If still no judgement can be inferred, raise an error
            raise ValueError(f"Unable to extract a valid judgement from the response: {raw_response[:100]}...")

    # Log the raw response and extracted judgement
    print(f"Raw response: {raw_response[:100]}...")
    print(f"Extracted judgement: {judgement}")

    return judgement


def rejection_sample_judgements(
    generated_judgements: list[str], ground_truth_judgements: list[str]
) -> list[str]:
    """
    Perform rejection sampling on generated judgements based on ground truth judgements.

    This function compares each generated judgement with its corresponding ground truth judgement
    and only keeps the generated judgements that agree with the ground truth.

    Args:
        generated_judgements (list[str]): A list of judgements generated by a model.
        ground_truth_judgements (list[str]): A list of ground truth judgements to compare against.

    Returns:
        list[str]: A list of generated judgements that agree with their corresponding ground truth judgements.
    """
    def do_judgements_agree(generated_judgement: str, ground_truth_judgement: str) -> bool:
        # For now the check is simple, but later it might be more complex.
        return generated_judgement == ground_truth_judgement

    return [
        generated_judgement
        for generated_judgement, ground_truth_judgement in zip(
            generated_judgements, ground_truth_judgements
        )
        if do_judgements_agree(generated_judgement, ground_truth_judgement)
    ]

def generate_preference_data(model, dataset: Dataset):
    # At this stage the dataset only needs to contain some user instructions.
    df = dataset.to_pandas()
    df[["chosen", "rejected", "modified_instruction"]] = pd.DataFrame(df.apply(
        lambda row: generate_response_pair_and_modified_instruction(model, row["instruction"]),
        axis=1
    ))

    # Let's assemble each datapoint into the prompt format used in the judgement annotation.
    df["prompt"] = df.apply(
        lambda row: PROMPT_TO_GENERATE_JUDGEMENT_ANNOTATION.format(
            instruction=row["instruction"],
            chosen=row["chosen"],
            rejected=row["rejected"],
        ),
        axis=1
    )

    # Now let's generate the judgements.
    df["judgement"] = df["prompt"].apply(
        lambda prompt: generate_judgement(model, prompt)
    )
    return Dataset.from_pandas(df)
